{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful notebooks:\n",
    "\n",
    "- Preprocessing : https://www.kaggle.com/code/motono0223/js24-preprocessing-create-lags\n",
    "- Training (XGB) : https://www.kaggle.com/code/motono0223/js24-train-gbdt-model-with-lags-singlemodel\n",
    "  - trained XGB model : https://www.kaggle.com/datasets/motono0223/js24-trained-gbdt-model\n",
    "- Training (NN): **this notebook** https://www.kaggle.com/code/voix97/jane-street-rmf-training-nn\n",
    "  - trained NN model : https://www.kaggle.com/datasets/voix97/js-xs-nn-trained-model\n",
    "- Inference of NN : https://www.kaggle.com/code/voix97/jane-street-rmf-nn-with-pytorch-lightning\n",
    "- Inference of NN+XGB:  https://www.kaggle.com/code/voix97/jane-street-rmf-nn-xgb\n",
    "- EDA(1) : https://www.kaggle.com/code/motono0223/eda-jane-street-real-time-market-data-forecasting\n",
    "- EDA(2) : https://www.kaggle.com/code/motono0223/eda-v2-jane-street-real-time-market-forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks (MLP) with PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:41:12.718619Z",
     "iopub.status.busy": "2025-10-10T04:41:12.717907Z",
     "iopub.status.idle": "2025-10-10T04:41:13.962475Z",
     "shell.execute_reply": "2025-10-10T04:41:13.961127Z",
     "shell.execute_reply.started": "2025-10-10T04:41:12.718568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:41:13.964998Z",
     "iopub.status.busy": "2025-10-10T04:41:13.964470Z",
     "iopub.status.idle": "2025-10-10T04:41:56.865685Z",
     "shell.execute_reply": "2025-10-10T04:41:56.864687Z",
     "shell.execute_reply.started": "2025-10-10T04:41:13.964953Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_321/2743497322.py:12: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
      "  schema = train_lazy.schema\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing validation data...\n",
      "Validation data shape: (1082224, 104)\n",
      "Loading, preprocessing, and concatenating training data...\n",
      "Combined training data shape: (22104280, 104)\n",
      "Data loading and preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Ultimate Memory-Optimized Data Loading Strategy ---\n",
    "print(\"Starting ultimate memory-optimized data loading...\")\n",
    "\n",
    "input_path = './input_df' if os.path.exists('./input_df') else '/kaggle/input/js24-preprocessing-create-lags'\n",
    "feature_names = [f\"feature_{i:02d}\" for i in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n",
    "label_name = 'responder_6'\n",
    "weight_name = 'weight'\n",
    "\n",
    "# 1. Load ONLY the validation set. It's small enough to be kept in memory for all folds.\n",
    "print(\"Loading and preprocessing validation data (to keep in memory)...\")\n",
    "valid_processed_lazy = (\n",
    "    pl.scan_parquet(f\"{input_path}/validation.parquet\")\n",
    "    .cast({col: pl.Float32 for col, dtype in pl.scan_parquet(f\"{input_path}/validation.parquet\").schema.items() if dtype == pl.Float64})\n",
    "    .with_columns(pl.col(feature_names).forward_fill().fill_null(0))\n",
    ")\n",
    "valid = valid_processed_lazy.collect().to_pandas()\n",
    "print(f\"Validation data loaded. Shape: {valid.shape}\")\n",
    "\n",
    "# 2. Get all unique date_ids from the combined dataset for cross-validation splitting.\n",
    "# This is a very fast and low-memory operation as it only scans one column.\n",
    "print(\"Scanning for all unique date_ids for fold splitting...\")\n",
    "all_dates_train = pl.scan_parquet(f\"{input_path}/training.parquet\").select('date_id').unique()\n",
    "all_dates_valid = pl.scan_parquet(f\"{input_path}/validation.parquet\").select('date_id').unique()\n",
    "all_dates = pl.concat([all_dates_train, all_dates_valid]).unique().collect().to_series().to_list()\n",
    "all_dates.sort()\n",
    "print(f\"Found {len(all_dates)} unique dates for cross-validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:41:59.839770Z",
     "iopub.status.busy": "2025-10-10T04:41:59.839483Z",
     "iopub.status.idle": "2025-10-10T04:42:07.521417Z",
     "shell.execute_reply": "2025-10-10T04:42:07.520392Z",
     "shell.execute_reply.started": "2025-10-10T04:41:59.839743Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning\n",
    "from pytorch_lightning import (LightningDataModule, LightningModule, Trainer)\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class custom_args():\n",
    "    def __init__(self):\n",
    "        self.usegpu = True\n",
    "        self.gpuid = 0\n",
    "        self.seed = 42\n",
    "        self.model = 'nn'\n",
    "        self.use_wandb = False\n",
    "        self.project = 'js-xs-nn-with-lags'\n",
    "        self.dname = \"./input_df/\"\n",
    "        self.loader_workers = 4\n",
    "        self.bs = 8192\n",
    "        self.lr = 1e-3\n",
    "        self.weight_decay = 5e-4\n",
    "        self.dropouts = [0.1, 0.1]\n",
    "        self.n_hidden = [512, 512, 256]\n",
    "        self.patience = 25\n",
    "        self.max_epochs = 2000\n",
    "        self.N_fold = 5\n",
    "\n",
    "\n",
    "my_args = custom_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Data Module Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:42:07.523120Z",
     "iopub.status.busy": "2025-10-10T04:42:07.522727Z",
     "iopub.status.idle": "2025-10-10T04:42:07.533416Z",
     "shell.execute_reply": "2025-10-10T04:42:07.532380Z",
     "shell.execute_reply.started": "2025-10-10T04:42:07.523074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Ultimate Memory-Optimized PyTorch Data Module ---\n",
    "import os\n",
    "\n",
    "# Define a worker_init_fn to print worker PIDs for verification\n",
    "def worker_init_fn(worker_id):\n",
    "    pid = os.getpid()\n",
    "    # This print statement will appear in the logs for each worker process\n",
    "    print(f\"SUCCESS: DataLoader Worker with ID {worker_id} started with PID: {pid}\")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Memory-efficient Dataset using Just-in-Time tensor conversion.\"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.features = df[feature_names].values\n",
    "        self.labels = df[label_name].values\n",
    "        self.weights = df[weight_name].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        w = torch.tensor(self.weights[idx], dtype=torch.float32)\n",
    "        return x, y, w\n",
    "\n",
    "class DataModule(LightningDataModule):\n",
    "    \"\"\"This DataModule loads data fold-by-fold from disk to minimize memory usage.\"\"\"\n",
    "    def __init__(self, batch_size, valid_df, all_dates):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.valid_df = valid_df\n",
    "        self.all_dates = all_dates\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.input_path = './input_df' if os.path.exists('./input_df') else '/kaggle/input/js24-preprocessing-create-lags'\n",
    "\n",
    "    def setup(self, fold=0, N_fold=5, stage=None):\n",
    "        print(f\"Setting up data for fold {fold}...\")\n",
    "        train_dates = [date for i, date in enumerate(self.all_dates) if i % N_fold != fold]\n",
    "        \n",
    "        print(f\"Lazily preparing training data for {len(train_dates)} dates for fold {fold}...\")\n",
    "        def process_lazy(lazy_frame):\n",
    "            schema = lazy_frame.schema\n",
    "            dtype_map = {col: pl.Float32 for col, dtype in schema.items() if dtype == pl.Float64}\n",
    "            return (\n",
    "                lazy_frame.filter(pl.col('date_id').is_in(train_dates))\n",
    "                .cast(dtype_map).with_columns(pl.col(feature_names).forward_fill().fill_null(0))\n",
    "            )\n",
    "\n",
    "        train_lazy_fold = process_lazy(pl.scan_parquet(f\"{self.input_path}/training.parquet\"))\n",
    "        valid_lazy_fold = process_lazy(pl.scan_parquet(f\"{self.input_path}/validation.parquet\"))\n",
    "        \n",
    "        df_train = pl.concat([train_lazy_fold, valid_lazy_fold]).collect().to_pandas()\n",
    "        print(f\"Fold {fold} training data loaded into memory. Shape: {df_train.shape}\")\n",
    "        \n",
    "        self.train_dataset = CustomDataset(df_train)\n",
    "        \n",
    "        if self.val_dataset is None:\n",
    "            print(\"Creating validation dataset (this happens only once)...\")\n",
    "            self.val_dataset = CustomDataset(self.valid_df)\n",
    "\n",
    "    def train_dataloader(self, n_workers=0):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=n_workers,\n",
    "            worker_init_fn=worker_init_fn # Add this to verify workers\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self, n_workers=0):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=n_workers,\n",
    "            worker_init_fn=worker_init_fn # Add this to verify workers\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:42:07.534967Z",
     "iopub.status.busy": "2025-10-10T04:42:07.534670Z",
     "iopub.status.idle": "2025-10-10T04:42:07.549486Z",
     "shell.execute_reply": "2025-10-10T04:42:07.548541Z",
     "shell.execute_reply.started": "2025-10-10T04:42:07.534922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Custom R2 metric for validation\n",
    "def r2_val(y_true, y_pred, sample_weight):\n",
    "    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n",
    "    return r2\n",
    "\n",
    "\n",
    "class NN(LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dims, dropouts, lr, weight_decay):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.BatchNorm1d(in_dim))\n",
    "            if i > 0:\n",
    "                layers.append(nn.SiLU())\n",
    "            if i < len(dropouts):\n",
    "                layers.append(nn.Dropout(dropouts[i]))\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            # layers.append(nn.ReLU())\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, 1)) \n",
    "        layers.append(nn.Tanh())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "        # Add a variable to store the start time of each epoch\n",
    "        self._epoch_start_time = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 5 * self.model(x).squeeze(-1)  \n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, y, w = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y, reduction='none') * w  #\n",
    "        loss = loss.mean()\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        x, y, w = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y, reduction='none') * w\n",
    "        loss = loss.mean()\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n",
    "        self.validation_step_outputs.append((y_hat, y, w))\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n",
    "        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "        if self.trainer.sanity_checking:\n",
    "            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "        else:\n",
    "            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "            # r2_val\n",
    "            val_r_square = r2_val(y, prob, weights)\n",
    "            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n",
    "                                                               verbose=True)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        \"\"\"Record the start time at the beginning of each training epoch.\"\"\"\n",
    "        self._epoch_start_time = time.time()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"Calculate and print the duration at the end of each training epoch.\"\"\"\n",
    "        if self.trainer.sanity_checking:\n",
    "            return\n",
    "        \n",
    "        # Calculate epoch duration\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - self._epoch_start_time\n",
    "        \n",
    "        epoch = self.trainer.current_epoch\n",
    "        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n",
    "        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n",
    "        \n",
    "        # Add epoch duration to the printed output\n",
    "        print(f\"Epoch {epoch}: {formatted_metrics} -- Duration: {epoch_duration:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PyTorch Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T04:42:07.550855Z",
     "iopub.status.busy": "2025-10-10T04:42:07.550554Z",
     "iopub.status.idle": "2025-10-10T04:42:07.695727Z",
     "shell.execute_reply": "2025-10-10T04:42:07.694962Z",
     "shell.execute_reply.started": "2025-10-10T04:42:07.550827Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Data Module with preprocessed data...\n",
      "Data Module initialized.\n"
     ]
    }
   ],
   "source": [
    "args = my_args\n",
    "\n",
    "# checking device\n",
    "device = torch.device(f'cuda:{args.gpuid}' if torch.cuda.is_available() and args.usegpu else 'cpu')\n",
    "accelerator = 'gpu' if torch.cuda.is_available() and args.usegpu else 'cpu'\n",
    "\n",
    "# Initialize Data Module with the new fold-by-fold loading strategy.\n",
    "# It now takes the pre-loaded 'valid' dataframe and the list of 'all_dates'.\n",
    "print(\"Initializing Data Module with the new fold-by-fold loading strategy...\")\n",
    "data_module = DataModule(batch_size=args.bs, valid_df=valid, all_dates=all_dates)\n",
    "print(\"Data Module initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# FOLD CONTROL\n",
    "# ---------------------------------------------------------\n",
    "# This is the only variable you need to change for each run.\n",
    "# Set this value from 0 to 4 to train each fold separately.\n",
    "# For your first run, keep it at 0.\n",
    "# For your second run, change it to 1, and so on.\n",
    "# =========================================================\n",
    "FOLD_TO_TRAIN = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-10T04:42:40.796Z",
     "iopub.execute_input": "2025-10-10T04:42:07.696870Z",
     "iopub.status.busy": "2025-10-10T04:42:07.696625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "# 确保保存模型的目录存在\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "\n",
    "# --- Memory-Optimized Single-Fold Training ---\n",
    "\n",
    "# This training block now runs for only ONE FOLD, controlled by FOLD_TO_TRAIN.\n",
    "# This makes each Kaggle run short and ensures the model output is saved correctly.\n",
    "\n",
    "pytorch_lightning.seed_everything(args.seed)\n",
    "\n",
    "# Set the current fold to the one specified in the control cell\n",
    "fold = FOLD_TO_TRAIN\n",
    "print(f\"\\n{'='*20} STARTING TRAINING FOR FOLD {fold} {'='*20}\")\n",
    "\n",
    "# The setup call will trigger memory-efficient loading for this specific fold.\n",
    "data_module.setup(fold, args.N_fold)\n",
    "\n",
    "input_dim = data_module.train_dataset.features.shape[1]\n",
    "model = NN(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dims=args.n_hidden,\n",
    "    dropouts=args.dropouts,\n",
    "    lr=args.lr,\n",
    "    weight_decay=args.weight_decay\n",
    ")\n",
    "\n",
    "if args.use_wandb:\n",
    "    wandb_run = wandb.init(project=args.project, config=vars(args), reinit=True)\n",
    "    logger = WandbLogger(experiment=wandb_run)\n",
    "else:\n",
    "    logger = None\n",
    "    \n",
    "early_stopping = EarlyStopping('val_loss', patience=args.patience, mode='min', verbose=True)\n",
    "# The filename now correctly uses our FOLD_TO_TRAIN variable to save the right model\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1, verbose=True, filename=f\"./models/nn_{fold}.model\") \n",
    "timer = Timer()\n",
    "\n",
    "# I've also added the speed optimizations we discussed earlier.\n",
    "# Note the comma after enable_progress_bar=True is now correctly added.\n",
    "trainer = Trainer(\n",
    "    max_epochs=args.max_epochs,\n",
    "    accelerator=accelerator,\n",
    "    devices=[args.gpuid] if args.usegpu else None,\n",
    "    logger=logger,\n",
    "    callbacks=[early_stopping, checkpoint_callback, timer],\n",
    "    enable_progress_bar=True,\n",
    "    precision='16-mixed' # Use mixed precision for speed\n",
    ")\n",
    "\n",
    "# Use multiple workers for faster data loading\n",
    "N_WORKERS = 4\n",
    "print(f\"Starting training for fold {fold} with num_workers={N_WORKERS}...\")\n",
    "trainer.fit(\n",
    "    model, \n",
    "    data_module.train_dataloader(n_workers=N_WORKERS), \n",
    "    data_module.val_dataloader(n_workers=N_WORKERS)\n",
    ")\n",
    "\n",
    "print(f\"Fold-{fold} Training completed in {timer.time_elapsed('train'):.2f}s\")\n",
    "\n",
    "# Clean up memory at the end of the run\n",
    "del model, trainer, checkpoint_callback, early_stopping, timer, data_module.train_dataset\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n{'='*20} SUCCESSFULLY FINISHED FOLD {fold} {'='*20}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11305158,
     "sourceId": 84493,
     "sourceType": "competition"
    },
    {
     "sourceId": 203900450,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.594014,
   "end_time": "2024-10-10T11:58:36.355301",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-10T11:58:28.761287",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
